{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 준비된 데이터들을 확인할 것 : 2가지 파일 [ train.csv, test.csv]\n",
    "* 참고 : 해당 데이터는 kaggle 부분에서 참고한 것임.\n",
    "\n",
    "* numpy.genfromtxt : Load data from a text file, with missing values handled as specified.\n",
    "numpy.genfromtxt(fname, dtype=<type 'float'>, comments='#', delimiter=None, skip_header=0, skip_footer=0, converters=None, missing_values=None, filling_values=None, usecols=None, names=None, excludelist=None, deletechars=None, replace_space='_', autostrip=False, case_sensitive=True, defaultfmt='f%i', unpack=None, usemask=False, loose=True, invalid_raise=True, max_rows=None)[source]\n",
    "* ref) https://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html\n",
    "\n",
    "* train.csv 파일을 직접 열어서 확인할 것!! 그 이후에 이 파일을 어떻게 불러들여야 하는지 옵션을 선택할 수 있음!\n",
    "* 참고) uint8의 형식으로 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "data/01_MNIST_Kaggle/train.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-766ea1432b73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmnist_train_data_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"data/01_MNIST_Kaggle/train.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist_train_data_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'uint8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[0;32m   1770\u001b[0m             \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1771\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1772\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1773\u001b[0m             \u001b[0mfid_ctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    621\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[0;32m    622\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s not found.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: data/01_MNIST_Kaggle/train.csv not found."
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "mnist_train_data_path = \"data/01_MNIST_Kaggle/train.csv\"\n",
    "\n",
    "train = np.genfromtxt(mnist_train_data_path, delimiter=',', skip_header=1).astype(np.dtype('uint8'))\n",
    "t2 = time.time()\n",
    "\n",
    "print (\"Process Time : \" + str(t2-t1))\n",
    "print (train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "mnist_test_data_path = \"data/01_MNIST_Kaggle/test.csv\"\n",
    "\n",
    "test = np.genfromtxt(mnist_test_data_path, delimiter=',', skip_header=1).astype(np.dtype('uint8'))\n",
    "t2 = time.time()\n",
    "\n",
    "print (\"Process Time : \" + str(t2-t1))\n",
    "print (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(train))\n",
    "print (len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 참고) 위의 방법은 처음에 로드할 경우에 많은 시간이 소요된다.\n",
    "* 이를 해결하기 위해서는 처음에 큰 파일을 로드하고 나서, pickle을 이용해서 변환을 하면 다음에는 읽는데 시간이 적게 소요됨.\n",
    "* ref ) \n",
    "import pickle\n",
    "with open('data/train_points.p', 'wb') as f:\n",
    "    pickle.dump(X, f)\n",
    "...\n",
    "with open('data/train_points.p', 'rb') as f:\n",
    "    X = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 분리\n",
    "* 받은 데이터가 실제 데이터가 의미하는 숫자(0~9) + 실제 데이터로 구성되어 있음.\n",
    "* 즉, 1개의 데이터의 값이 일렬로 시리얼로 들어가 있는데, 맨 앞의 값이 label이고, 그 뒤의 값이 실제 데이터에 관련된 것임\n",
    "* 이를 통해서 라벨 + 데이터로 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train[:,1:]\n",
    "train_labels = train[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고로 Kaggle Data의 경우에는 label을 사이트에 올려야 정확도를 확인하게 되어 있어서 train하고 달리 순수한 data만 있음.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0~9까지 데이터가 다 들어있는지 확인!\n",
    "* 실제로 우리가 학습 시킬 데이터가 0~9까지 다 제대로 들어있는 데이터인지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 시키려는 데이터의 분포가 균일하게 적당히 분포가 되었는지 확인!!!!\n",
    "[   0 4132]\n",
    " [   1 4684]\n",
    " [   2 4177]\n",
    " [   3 4351]\n",
    " [   4 4072]\n",
    " [   5 3795]\n",
    " [   6 4137]\n",
    " [   7 4401]\n",
    " [   8 4063]\n",
    " [   9 4188]\n",
    " * 위와 같은 결과가 나오는지 확인!!!(단, 방법은 여러가지임!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method _ 01) numpy의 unique이용해서 처리\n",
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실제 데이터 값을 이미지로 확인해보자!\n",
    "* ref) http://yann.lecun.com/exdb/mnist/\n",
    "* 위의 사이트에 들어가서 실제 데이터의 구성을 확인해보자!!! 실제 내용을 보면 원래 데이터는 28 X 28 크기의 이미지로 이루어진 것을 알 수 있다. 다만, 우리가 받아들이고, 데이터화 하는 과정에서 label이 부여되었고, 처리를 위해서 matrix가 아닌 28x28 크기의 1차원 어레이로 처리가 된 것을 볼 수 있다.\n",
    "\n",
    "* 시각화 부분 : http://seaborn.pydata.org/generated/seaborn.heatmap.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 앞에서 label은 분리하였으니, 순수 데이터에서 크기 부분을 확인하자!!!\n",
    "print(train_data[0].shape)\n",
    "print(np.sqrt(train_data[0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test[0].shape)\n",
    "print(np.sqrt(test[0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 보고자 하는 데이터의 index\n",
    "idx = 0\n",
    "\n",
    "image_data = train_data[idx]\n",
    "\n",
    "# 1차원이기 때문에 실제 값을 계산하기 위해서는 1차원의 길이값 획득 & 이를 제곱근으로 변환!!\n",
    "tot_size = image_data.shape[0]\n",
    "one_size = int(np.sqrt(tot_size))\n",
    "\n",
    "# 그림으로 표현\n",
    "sns.heatmap(np.reshape(image_data, (one_size, one_size)))\n",
    "plt.title(str(\"MNIST Digit Image _ Train Data Number : \")+str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(np.reshape(image_data, (one_size, one_size)), linewidth=0.1, xticklabels=False, yticklabels=False)\n",
    "plt.title(str(\"MNIST Digit Image _ Train Data Number : \")+str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터의 실제 값을 보고 싶을 때!!!\n",
    "sns.heatmap(np.reshape(image_data, (one_size, one_size)), linewidth=0, xticklabels=True, yticklabels=True, annot=True)\n",
    "plt.title(str(\"MNIST Digit Image _ Train Data Number : \")+str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고) xticklabels, yticklabels 중에서 하나라도 F 이면 annot 기능이 활성화 되지 않는다;;;\n",
    "sns.heatmap(np.reshape(image_data, (one_size, one_size)), linewidth=0, xticklabels=False, yticklabels=True, annot=True)\n",
    "plt.title(str(\"MNIST Digit Image _ Train Data Number : \")+str(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "# edited 2018\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주의!!) 생각보다 데이터의 양이 많기 때문에 시간이 오래걸릴 수 있음!!!\n",
    "# Process Time : 1648.52247596\n",
    "\n",
    "t1= time.time()\n",
    "\n",
    "# Default k=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\n",
    "\n",
    "# Data Fitting\n",
    "knn.fit(train_data, train_labels)\n",
    "\n",
    "# Data Prediction\n",
    "pred = knn.predict(test)\n",
    "t2 = time.time()\n",
    "print(\"Process Time : \" + str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제로 결과가 나타났는지 확인\n",
    "print(len(pred))\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kaggle 에서는 그냥 업로드를 하는 것이 아니라 특정한 가이드가 있음. 이는 변경될 수 있으니, 해당 정보들을 확인하고 할 것!!\n",
    "* 여기서는 csv 양식으로 업로드를 해야하고, 컬럼 이름이 ImageId, Label 이고, test 데이터의 값들과 그 옆에 모델을 통해 예측한 값을 기록하여 업로드를 해야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력용 만들기\n",
    "#img_id = []\n",
    "# sumit 관련해서는 id가 1부터 시작되어야 하니, index matching 부분임.\n",
    "#for i in range(1, len(test)+1):\n",
    " #   img_id.append(i)\n",
    "#predMNIST = pd.DataFrame({ \"ImageId\": img_id,\"Label\": pred})   \n",
    "# outPath\n",
    "#out_path = \"/Users/minkyukwon/Desktop/에이콘강의/ML_resource/01_knn/01_MNIST/mnist_pred.csv\"\n",
    "#predMNIST.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실제로 Kaggle에 Summit 하기!!!\n",
    "* ref) https://www.kaggle.com/c/digit-recognizer/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = u\"/Users/minkyukwon/PycharmProjects/MachineLearning_Course/10_knn/ref_img/kaggle_test_submit.png\"\n",
    "from IPython.display import Image\n",
    "Image(filename=img_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 위의 데이터를 PCA를 적용해서 해보자!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단순히 앞의 코드를 재사용하기 위해서 변수명 변경\n",
    "test_data = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_comp = 60\n",
    "pca = PCA(n_components=num_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine amount of variance explained by components\n",
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the explained variance\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.title('Variance Explained by Extracted Componenent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the features\n",
    "train_ext = pca.fit_transform(train_data)\n",
    "print(train_ext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the test data using the existing parameters\n",
    "test_ext = pca.transform(test_data)\n",
    "print(test_ext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_norm = min_max_scaler.fit_transform(train_ext)\n",
    "test_norm = min_max_scaler.fit_transform(test_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_norm, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(knn, train_norm, train_labels, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_with_pca = knn.predict(test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력용 만들기\n",
    "img_id = []\n",
    "# sumit 관련해서는 id가 1부터 시작되어야 하니, index matching 부분임.\n",
    "for i in range(1, len(test)+1):\n",
    "    img_id.append(i)\n",
    "predMNIST = pd.DataFrame({ \"ImageId\": img_id,\"Label\": pred})   \n",
    "# outPath\n",
    "out_path = \"/Users/minkyukwon/Desktop/에이콘강의/ML_resource/01_knn/01_MNIST/mnist_pred.csv\"\n",
    "predMNIST.to_csv(out_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = u\"/Users/minkyukwon/PycharmProjects/MachineLearning_Course/10_knn/ref_img/kaggle_test_submit_withPCA.png\"\n",
    "from IPython.display import Image\n",
    "Image(filename=img_path) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
